[
  {
    "type": "text",
    "id": "title",
    "x": 2400,
    "y": -300,
    "width": 800,
    "height": 100,
    "content": "# DeepSeek-R1 训练流程\n\n## 开源混合推理模型"
  },
  {
    "type": "text",
    "id": "intro",
    "x": 1500,
    "y": -150,
    "width": 2600,
    "height": 200,
    "content": "### 概述\n\nDeepSeek-R1 采用**两阶段**训练方法：\n1. **DeepSeek-R1-Zero**：仅使用强化学习（RL），探索推理能力涌现\n2. **DeepSeek-R1**：完整版本，结合冷启动数据和多轮迭代训练\n\n**核心算法**：Group Relative Policy Optimization (GRPO)\n- **优势**：无需 critic 模型，显著减少内存消耗\n- **两种奖励**：规则奖励（代码执行）+ 语言模型奖励（偏好对）"
  },
  {
    "type": "text",
    "id": "zero_title",
    "x": 200,
    "y": 100,
    "width": 1500,
    "height": 200,
    "content": "## 阶段 0: DeepSeek-R1-Zero\n\n### 🎯 目标\n探索在大规模基础模型上直接应用 RL 是否能涌现推理能力\n\n### ✅ 特点\n- **无需任何 SFT 数据**\n- 直接在 DeepSeek-V3-Base 上应用 RL\n- 使用 GRPO 算法\n\n### 🔍 关键发现\n1. **推理模式涌现**：AIME 评分从 15.6% → 71.0%\n2. **自我进化**：自发产生多种推理模式（反思、验证）\n3. **长思维链**：平均 60.6K tokens（远超传统模型）\n\n### ⚠️ 问题\n- 语言混合（中英混杂）\n- 思维链重复\n- 可读性差"
  },
  {
    "type": "text",
    "id": "step1",
    "x": 2100,
    "y": 100,
    "width": 1500,
    "height": 280,
    "content": "## 步骤 1: 冷启动数据 (Cold Start)\n\n### 📋 目标\n解决 R1-Zero 的语言混合和可读性问题\n\n### 🔧 实施方法\n1. **收集少量高质量数据**\n   - 数千条精心设计的 prompt\n2. **使用 DeepSeek-V3 生成回答**\n   - 保证语言纯度（纯中文/英文）\n   - 确保思维链清晰可读\n3. **数据质量保证**\n   - 人工审核\n   - 去除重复内容\n   - 格式规范统一\n\n### 💡 作用\n为后续 SFT 和 RL 提供**高质量起点**\n\n### 📊 数据量\n约 **数千条**，远小于传统 SFT 数据集"
  },
  {
    "type": "text",
    "id": "step2_intro",
    "x": 4000,
    "y": 100,
    "width": 1500,
    "height": 200,
    "content": "## 步骤 2: 迭代训练 (核心)\n\n### 🔄 多轮循环\nDeepSeek-R1 经过 **2 轮** 完整迭代\n\n每轮包含：\n- **阶段 A**：推理 focused SFT\n- **阶段 B**：推理 focused RL\n- **阶段 C**：Rejection Sampling + SFT\n- **阶段 D**：再次 RL\n\n### 📈 持续改进\n每轮迭代后模型性能显著提升\n\n### 🎓 最终目标\n与 DeepSeek-V3 对齐并保持通用能力"
  },
  {
    "type": "text",
    "id": "stage_a",
    "x": 1900,
    "y": 450,
    "width": 1500,
    "height": 280,
    "content": "### 阶段 A: 推理 Focused SFT\n\n#### 🎯 目标\n让模型学习**推理任务**的思考模式\n\n#### 🔧 训练方法\n1. **使用冷启动数据**\n   - DeepSeek-V3 生成的数千条高质量数据\n2. **微调基础模型**\n   - 模型：DeepSeek-V3-Base\n   - 任务：学习推理模式\n3. **学习目标**\n   - 逐步思考\n   - 尝试不同策略\n   - 验证答案\n\n#### 📊 数据特点\n- **推理密集**：数学、代码、逻辑推理\n- **思维链展示**：包含完整的思考过程\n- **任务多样**：覆盖多个推理场景"
  },
  {
    "type": "text",
    "id": "stage_b",
    "x": 3800,
    "y": 450,
    "width": 1800,
    "height": 400,
    "content": "### 阶段 B: 推理 Focused RL (GRPO)\n\n#### 🎯 目标\n通过强化学习提升推理能力\n\n#### 🔧 GRPO 算法优势\n- **无需 Critic 模型**：通过 group baseline 估计优势\n- **内存高效**：显著减少训练时的内存消耗\n- **适合长序列**：支持 60K+ tokens 的思维链\n\n#### 🏆 双奖励机制\n**1. 规则奖励 (Rule-based Rewards)**\n- **代码执行结果**：通过编译/测试验证\n- **数学答案匹配**：与标准答案比对\n- **客观可验证**：无需人工标注\n\n**2. 语言模型奖励**\n- **偏好对训练**：收集模型输出偏好数据\n- **质量判断**：训练奖励模型评估输出质量\n- **通用能力保持**：防止模型过度专精推理\n\n#### 📈 RL 规模\n- **大规模部署**：数千 GPU 并行训练\n- **长训练时间**：数千小时的 RL 训练"
  },
  {
    "type": "text",
    "id": "stage_c",
    "x": 2100,
    "y": 800,
    "width": 1600,
    "height": 350,
    "content": "### 阶段 C: Rejection Sampling + SFT\n\n#### 🎯 目标\n从 RL 模型中提取**高质量推理数据**\n\n#### 🔧 Rejection Sampling 流程\n1. **生成多个输出**\n   - 对每个 prompt 生成 60+ 个不同回答\n   - 温度参数控制多样性\n2. **质量筛选**\n   - 使用奖励模型评分\n   - 选择最优 1-2 个输出\n3. **构建 SFT 数据集**\n   - 仅保留高质量推理链\n   - 确保语言纯度和可读性\n\n#### 💡 作用\n- **数据蒸馏**：将 RL 学到的策略固化为 SFT 数据\n- **质量提升**：每轮迭代后数据质量显著提高\n- **训练稳定**：SFT 提供稳定的训练信号\n\n#### 📊 数据规模\n每轮生成数百万候选，最终保留数十万高质量样本"
  },
  {
    "type": "text",
    "id": "stage_d",
    "x": 4100,
    "y": 900,
    "width": 1500,
    "height": 280,
    "content": "### 阶段 D: 再次 RL (第2轮)\n\n#### 🎯 目标\n在新 SFT 数据上进一步提升推理能力\n\n#### 🔧 训练方法\n1. **使用阶段 C 的 SFT 数据**\n   - 更新模型为最新 SFT 版本\n2. **再次应用 GRPO**\n   - 相同的双奖励机制\n3. **探索新策略**\n   - 尝试不同的推理路径\n   - 发现更优的解决方案\n\n#### 📈 性能提升\n- **AIME**: 71.0% → 86.7% (R1-Zero → R1)\n- **MATH**: 83.8% → 92.0%\n- **Codeforces**: 13.5% → 45.0%\n\n#### 🔄 循环迭代\n可继续阶段 C → 阶段 D 循环，但 **2 轮**已达到平衡"
  },
  {
    "type": "text",
    "id": "step3",
    "x": 200,
    "y": 500,
    "width": 1400,
    "height": 350,
    "content": "## 步骤 3: 通用能力对齐\n\n### 🎯 目标\n在强化推理能力的同时**保持通用能力**\n\n### 📋 SFT 数据构造\n创建多场景指令微调数据：\n\n#### 🔬 推理场景\n- 数学问题求解\n- 代码编写与调试\n- 逻辑推理任务\n- **特点**：展示思维链\n\n#### ✍️ 写作场景\n- 创意写作\n- 信息整理\n- 问答对话\n- **特点**：**无**思维链，直接输出\n\n#### 🔧 数据平衡\n- **推理任务**：需要思考的任务展示推理过程\n- **非推理任务**：直接输出答案，避免过度推理\n\n### 📊 训练规模\n- **数据量**：数十万条指令微调数据\n- **模型**：DeepSeek-V3-Base\n- **结果**：R1-Distill 模型系列"
  },
  {
    "type": "text",
    "id": "models",
    "x": 200,
    "y": 950,
    "width": 1500,
    "height": 400,
    "content": "## 模型系列\n\n### 🎯 DeepSeek-R1 (671B)\n**完整模型**，最佳性能\n- **训练**：完整流程\n- **部署**：API 服务\n\n### 🔄 DeepSeek-R1-Zero (671B)\n**纯 RL 训练**，无冷启动\n- **用途**：研究 RL 涌现现象\n- **特点**：语言混合、思维链更原始\n\n### 💚 DeepSeek-R1-Distill 系列\n基于 Qwen 和 Llama 架构的蒸馏模型\n\n| 模型 | 参数量 | 基座模型 | 用途 |\n|------|--------|----------|------|\n| R1-Distill-Qwen-1.5B | 1.5B | Qwen-1.5B | 轻量部署 |\n| R1-Distill-Qwen-7B | 7B | Qwen-7B | 边缘设备 |\n| R1-Distill-Qwen-8B | 8B | Qwen-8B | 平衡性能 |\n| R1-Distill-Qwen-14B | 14B | Qwen-14B | 中等规模 |\n| R1-Distill-Qwen-32B | 32B | Qwen-32B | 高性能 |\n| R1-Distill-Llama-8B | 8B | Llama-3.1-8B | Llama 生态 |\n| R1-Distill-Llama-70B | 70B | Llama-3.3-70B | 顶级性能 |\n\n#### 💡 蒸馏方法\n使用 R1 生成的 **800K 训练样本** 微调开源基座"
  },
  {
    "type": "text",
    "id": "results",
    "x": 1900,
    "y": 1250,
    "width": 1800,
    "height": 450,
    "content": "## 🏆 性能表现\n\n### 推理能力 (与 OpenAI o1 比较)\n\n| 基准 | DeepSeek-R1 | OpenAI o1-1217 |\n|------|-------------|----------------|\n| **AIME 2024** | 79.8% (63.6%) | 77.5% (72.3%) |\n| **MATH-500** | 97.3% | 96.4% |\n| **Codeforces** | 48.5% | 48.0% |\n| **LiveCodeBench** | 53.2% | 53.0% |\n| **GPQA Diamond** | 59.1% | 58.3% |\n\n### 通用能力\n\n| 基准 | DeepSeek-R1 | GPT-4o | Claude-3.5-Sonnet |\n|------|-------------|--------|------------------|\n| **MMLU** | 89.8% | 87.6% | 88.7% |\n| **MMLU-Pro** | 84.0% | 79.8% | 79.2% |\n| **MMLU-Redux** | 91.5% | 88.8% | 88.2% |\n| **C-Eval** | 89.2% | 84.5% | 84.7% |\n| **GSM8K** | 96.3% | 92.0% | 96.4% |\n\n### 💡 关键优势\n1. **开源透明**：完全公开训练方法和数据\n2. **成本效益**：仅用训练 GPT-4o 约 **1/20** 的成本\n3. **通用能力强**：在强化推理的同时保持甚至提升通用能力\n4. **可蒸馏**：成功蒸馏到小模型，性能接近甚至超越原始基座"
  },
  {
    "type": "text",
    "id": "innovation",
    "x": 4100,
    "y": 1250,
    "width": 1700,
    "height": 400,
    "content": "## 🔬 技术创新\n\n### 1️⃣ GRPO 算法\n- **Group Relative Policy Optimization**\n- **优势**：无需 critic 模型，减少内存消耗\n- **原理**：通过 group sampling 估计 baseline\n\n### 2️⃣ 双奖励机制\n**规则奖励 + 语言模型奖励**：\n- **规则**：处理可验证任务（代码、数学）\n- **LM 奖励**：保持通用能力，处理不可验证任务\n\n### 3️⃣ 多轮迭代策略\n**SFT → RL → Rejection Sampling → SFT → RL**：\n- 每轮迭代后质量显著提升\n- 2 轮达到推理与通用能力的平衡\n\n### 4️⃣ 任务感知推理\n**推理任务展示思维链，非推理任务直接输出**：\n- 避免过度推理\n- 提升用户体验\n- 保持响应效率\n\n### 5️⃣ 知识蒸馏\n**将 671B 模型能力转移到小模型**：\n- 800K 高质量训练样本\n- 覆盖 Qwen 和 Llama 架构\n- 小模型也能展现强大推理能力"
  },
  {
    "type": "text",
    "id": "conclusion",
    "x": 200,
    "y": 1450,
    "width": 3400,
    "height": 250,
    "content": "## 🎓 总结与启示\n\n### ✅ 成功要素\n1. **最小化人工干预**：仅需数千条冷启动数据\n2. **强化学习为核心**：GRPO 算法高效提升推理能力\n3. **迭代改进**：多轮 SFT + RL + Rejection Sampling\n4. **通用能力保持**：双奖励机制和任务感知训练\n\n### 🚀 未来方向\n- 探索更多轮次迭代的收益\n- 优化 Rejection Sampling 效率\n- 扩展到更多推理任务类型\n- 改进多语言混合问题\n\n### 💪 里程碑意义\n- **证明纯 RL 可涌现推理能力**（R1-Zero）\n- **开源最强推理模型**之一\n- **为社区提供可复现的训练路径**"
  },
  {
    "type": "text",
    "id": "resources",
    "x": 4100,
    "y": 1700,
    "width": 1700,
    "height": 280,
    "content": "## 📚 资源链接\n\n### 官方资源\n- **论文**: [DeepSeek-R1 Technical Report](https://github.com/deepseek-ai/DeepSeek-R1)\n- **代码**: [GitHub Repository](https://github.com/deepseek-ai/DeepSeek-R1)\n- **模型**: [Hugging Face](https://huggingface.co/deepseek-ai)\n\n### 模型下载\n- **R1 (671B)**: `deepseek-ai/DeepSeek-R1`\n- **R1-Zero**: `deepseek-ai/DeepSeek-R1-Zero`\n- **Distill 系列**: `deepseek-ai/DeepSeek-R1-Distill-*`\n\n### 关键贡献\n- 完全开源的训练方法和数据\n- 多尺寸蒸馏模型\n- 透明可复现的实验"
  },
  {
    "id": "arrow_zero_to_step1",
    "type": "arrow",
    "x": 1700,
    "y": 200,
    "width": 400,
    "height": 100,
    "label": "发现问题：\n语言混合、\n可读性差",
    "color": "#e03131",
    "labelPosition": "top"
  },
  {
    "id": "arrow_step1_to_step2",
    "type": "arrow",
    "x": 3600,
    "y": 200,
    "width": 400,
    "height": 100,
    "label": "进入\n迭代训练",
    "color": "#2f9e44",
    "labelPosition": "top"
  },
  {
    "id": "arrow_a_to_b",
    "type": "arrow",
    "x": 3400,
    "y": 550,
    "width": 400,
    "height": 100,
    "label": "强化学习\nGRPO",
    "color": "#1971c2",
    "labelPosition": "top"
  },
  {
    "id": "arrow_b_to_c",
    "type": "arrow",
    "x": 3400,
    "y": 700,
    "width": 600,
    "height": 200,
    "label": "Rejection Sampling\n提取高质量数据",
    "color": "#f08c00",
    "labelPosition": "right"
  },
  {
    "id": "arrow_c_to_d",
    "type": "arrow",
    "x": 3400,
    "y": 900,
    "width": 700,
    "height": 150,
    "label": "新 SFT 数据\n下一轮 RL",
    "color": "#9c36b5",
    "labelPosition": "bottom"
  },
  {
    "id": "arrow_step2_to_step3",
    "type": "arrow",
    "x": 3700,
    "y": 1200,
    "width": 700,
    "height": 150,
    "label": "迭代完成\n进入对齐阶段",
    "color": "#2f9e44",
    "labelPosition": "right"
  }
]
